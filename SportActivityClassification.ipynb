{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044bfeaa",
   "metadata": {},
   "source": [
    "# Comparison of common Machine Learning Algorithms considering a sport activities Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceda949",
   "metadata": {},
   "source": [
    "Even if Deep Learning is continuosly growing in terms of importance, classical Machine Learning technquis still represent a cornerstone for people who approach computer science for the first time. There are loads of different techniques, some of them are specifically devoted to tackle given types of problems; some others are instead multi-porpose.\n",
    "\n",
    "One popular task is the so called *classification problem*, where the modelâ€™s output is a category with a semantic meaning. A classification model attempts to draw some conclusion from observed values.\n",
    "Different methods can be implemented to tackle this problem.\n",
    "\n",
    "Our focus is a brief comparative study over four different machine learning supervised techniques:\n",
    "1. Logistic Regression\n",
    "2. K Nearest Neighbors\n",
    "3. Decision Trees\n",
    "4. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c46f5",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838d0cc",
   "metadata": {},
   "source": [
    "Since the [dataset](https://archive-beta.ics.uci.edu/ml/datasets/daily+and+sports+activities) used for this study is organised in folders and subfolders according to a hierarchical scheme, a quick preprocessing operation aimed to creade a huge database is necessary. The idea is to extract the files from each folder and stack them together in a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68f6440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395e1af",
   "metadata": {},
   "source": [
    "The structure is organised in 19 folder (one for each activity), each one containing 8 folder (one for each person), again containing 60 text files (each represents 5 sec of sampling). Therefore, some indexes are initilized to refer to each of the aforementioned elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01404112",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIndex = [\"%02d\" % x for x in range(1,61)]\n",
    "personIndex = [\"%01d\" % x for x in range(1,9)]\n",
    "activityIndex = [\"%02d\" % x for x in range(1,20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ae3e5",
   "metadata": {},
   "source": [
    "Each file representing 5 sec sampling contains 125 rows (sampling frequency of 25Hz), hence for gain some representative values of those 5 sec time span, the average and the variance of all the recorder values are computed and combined into a single array. In this way, there are now 60 arrays for each person and activity which can be joined together. In addition, in the last column, the index of the acitivy, which ranges from 1 up to 19, is added.\n",
    "To clearly undestand the final CSV file, the headers of each column are implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetHeader = [\"T_xacc\", \"T_yacc\", \"T_zacc\", \"T_xgyro\", \"T_ygyro\", \"T_zgyro\", \"T_xmag\", \"T_ymag\", \"T_zmag\",\n",
    "        \"RA_xacc\", \"RA_yacc\", \"RA_zacc\", \"RA_xgyro\", \"RA_ygyro\", \"RA_zgyro\", \"RA_xmag\", \"RA_ymag\", \"RA_zmag\",\n",
    "        \"LA_xacc\", \"LA_yacc\", \"LA_zacc\", \"LA_xgyro\", \"LA_ygyro\", \"LA_zgyro\", \"LA_xmag\", \"LA_ymag\", \"LA_zmag\",\n",
    "        \"RL_xacc\", \"RL_yacc\", \"RL_zacc\", \"RL_xgyro\", \"RL_ygyro\", \"RL_zgyro\", \"RL_xmag\", \"RL_ymag\", \"RL_zmag\",\n",
    "        \"LL_xacc\", \"LL_yacc\", \"LL_zacc\", \"LL_xgyro\", \"LL_ygyro\", \"LL_zgyro\", \"LL_xmag\", \"LL_ymag\", \"LL_zmag\",\n",
    "        \"var_T_xacc\", \"var_T_yacc\", \"var_T_zacc\", \"var_T_xgyro\", \"var_T_ygyro\", \"var_T_zgyro\", \"var_T_xmag\", \"var_T_ymag\", \"var_T_zmag\",\n",
    "        \"var_RA_xacc\", \"var_RA_yacc\", \"var_RA_zacc\", \"var_RA_xgyro\", \"var_RA_ygyro\", \"var_RA_zgyro\", \"var_RA_xmag\", \"var_RA_ymag\", \"var_RA_zmag\",\n",
    "        \"var_LA_xacc\", \"var_LA_yacc\", \"var_LA_zacc\", \"var_LA_xgyro\", \"var_LA_ygyro\", \"var_LA_zgyro\", \"var_LA_xmag\", \"var_LA_ymag\", \"var_LA_zmag\",\n",
    "        \"var_RL_xacc\", \"var_RL_yacc\", \"var_RL_zacc\", \"var_RL_xgyro\", \"var_RL_ygyro\", \"var_RL_zgyro\", \"var_RL_xmag\", \"var_RL_ymag\", \"var_RL_zmag\",\n",
    "        \"var_LL_xacc\", \"var_LL_yacc\", \"var_LL_zacc\", \"var_LL_xgyro\", \"var_LL_ygyro\", \"var_LL_zgyro\", \"var_LL_xmag\", \"var_LL_ymag\", \"var_LL_zmag\",\n",
    "        \"activity_index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7dd2e",
   "metadata": {},
   "source": [
    "The entire dataset is made of 9120 arrays containing 91 elements: 3 senosors measuring along 3 different axis for 5 sensing locations, then 45 variance vlaues, plus the acitivity index at the end. To extract and collect all the data, a set of nested for loops is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the entire dataset: 9120 x 91\n",
    "# Person from 1 to 8\n",
    "allActivities = []\n",
    "print(\"Importing all data\")\n",
    "for k in range(19):\n",
    "    print(\"Elaborating activity number: \", activityIndex[k])\n",
    "    for j in range(8):\n",
    "        print(\"Elaborating person number: \", personIndex[j], end = \"\\r\")\n",
    "        for i in range(60):\n",
    "            filename = f\"./sportsDataset/a{activityIndex[k]}/p{personIndex[j]}/s{fileIndex[i]}.txt\"\n",
    "\n",
    "            data = np.loadtxt(filename, delimiter=',', skiprows=1, dtype=float)\n",
    "            dataT = data.transpose()            \n",
    "            average = np.mean(dataT, axis = 1)\n",
    "            variance = np.var(dataT, axis = 1)\n",
    "            \n",
    "            index = np.array([int(activityIndex[k])])\n",
    "            #newData = np.append(average,variance, index, axis = 0)\n",
    "            newData = np.concatenate((average, variance, index), axis = None)\n",
    "            \n",
    "            allActivities.append(newData)\n",
    "\n",
    "print(\"\\nData correctly stored\")\n",
    "\n",
    "ActivitiesDataset = np.array(allActivities)\n",
    "np.savetxt(\"./sportsDataset/ActivitiesDataset.csv\", ActivitiesDataset, delimiter=\",\", header = ','.join(datasetHeader), comments='')\n",
    "print(\"Activity dataset created\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43642b5",
   "metadata": {},
   "source": [
    "For training all the models, a specific dataset is created considering the data of the first 7 people. A similar approach is the used for its creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train dataset: 7980 x 91\n",
    "# Person from 1 to 7\n",
    "trainingActivities = []\n",
    "print(\"Importing training data\")\n",
    "for k in range(19):\n",
    "    print(\"Elaborating activity number: \", activityIndex[k])\n",
    "    for j in range(7):\n",
    "        print(\"Elaborating person number: \", personIndex[j], end = \"\\r\")\n",
    "        for i in range(60):\n",
    "            filename = f\"./sportsDataset/a{activityIndex[k]}/p{personIndex[j]}/s{fileIndex[i]}.txt\"\n",
    "\n",
    "            data = np.loadtxt(filename, delimiter=',', skiprows=1, dtype=float)\n",
    "            dataT = data.transpose()            \n",
    "            average = np.mean(dataT, axis = 1)\n",
    "            variance = np.var(dataT, axis = 1)\n",
    "            \n",
    "            index = np.array([int(activityIndex[k])])\n",
    "            #newData = np.append(average,variance, index, axis = 0)\n",
    "            newData = np.concatenate((average, variance, index), axis = None)\n",
    "            \n",
    "            trainingActivities.append(newData)\n",
    "\n",
    "print(\"\\nTraining data correctly stored\")\n",
    "\n",
    "activitiesTrainDataset = np.array(trainingActivities)\n",
    "np.savetxt(\"./sportsDataset/TrainingDataset.csv\", activitiesTrainDataset, delimiter=\",\", header = ','.join(datasetHeader), comments='')\n",
    "print(\"Training dataset created\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7566121",
   "metadata": {},
   "source": [
    "While for training all the models, the data regarding the 8th person are used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset: 1140 x 91\n",
    "# Person 8\n",
    "testingActivities = []\n",
    "print(\"Importing testing data\")\n",
    "for k in range(19):\n",
    "    print(\"Elaborating activity number: \", activityIndex[k])\n",
    "    print(\"Elaborating person number: \", personIndex[7], end = \"\\r\")\n",
    "    for i in range(60):\n",
    "        filename = './sportsDataset/a' + activityIndex[k] + '/p'+ personIndex[7] + '/s' + fileIndex[i] + '.txt'\n",
    "\n",
    "        data = np.loadtxt(filename, delimiter=',', skiprows=1, dtype=float)\n",
    "        dataT = data.transpose()            \n",
    "        average = np.mean(dataT, axis = 1)\n",
    "        variance = np.var(dataT, axis = 1)\n",
    "        \n",
    "        index = np.array([int(activityIndex[k])])\n",
    "        #newData = np.append(average, variance, index, axis = 0)\n",
    "        newData = np.concatenate((average, variance, index), axis = None)\n",
    "        \n",
    "        testingActivities.append(newData)\n",
    "\n",
    "print(\"\\nTesting data correctly stored\")\n",
    "\n",
    "activitiesTestDataset = np.array(testingActivities)\n",
    "np.savetxt(\"./sportsDataset/TestDataset.csv\", activitiesTestDataset, delimiter=\",\", header = ','.join(datasetHeader), comments='')\n",
    "print(\"Testing dataset created\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02148bd",
   "metadata": {},
   "source": [
    "Now, all the necessary databases are saved as CSV files and ready to feed the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2c2db",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295b71a",
   "metadata": {},
   "source": [
    "Logistic regression is the right algorithm to start with classification algorithms. Even though, the name â€˜Regressionâ€™ comes up, it is not a regression model, but a classification model. It uses a logistic function to frame binary output model. The output of the logistic regression will be a probability (0â‰¤xâ‰¤1), and can be used to predict the binary 0 or 1 as the output ( if x<0.5, output= 0, else output=1).\n",
    "\n",
    "**Loss function**\n",
    "\n",
    "We use **cross entropy** as our loss function. The basic logic here is that, whenever my prediction is badly wrong, (eg : yâ€™ =1 & y = 0), cost will be -log(0) which is infinity.\n",
    "\n",
    "**Advantages**\n",
    "-   Easy, fast and simple classification method.\n",
    "-   Î¸ parameters explains the direction and intensity of significance of independent variables over the dependent variable.\n",
    "-   Can be used for multiclass classifications also.\n",
    "-   Loss function is always convex.\n",
    "\n",
    "**Disadvantages**\n",
    "-   Cannot be applied on non-linear classification problems.\n",
    "-   Proper selection of features is required.\n",
    "-   Good signal to noise ratio is expected.\n",
    "-   Colinearity and outliers tampers the accuracy of LR model.\n",
    "\n",
    "**Hyperparameters**\n",
    "Logistic regression hyperparameters are mainly two: Learning rate(Î±) and Regularization parameter(Î»). Those have to be tuned properly to achieve high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f3bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7214ab",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3db04",
   "metadata": {},
   "source": [
    "K-nearest neighbors is a non-parametric method used for classification and regression. It is one of the most used ML techniques. It is a lazy learning model, with local approximation.\n",
    "\n",
    "**Advantages**\n",
    "-   Easy and simple machine learning model.\n",
    "-   Few hyperparameters to tune.\n",
    "\n",
    " **Disadvantages**\n",
    "-   k should be wisely selected.\n",
    "-   Large computation cost during runtime if sample size is large.\n",
    "-   Proper scaling should be provided for fair treatment among features.\n",
    "\n",
    "**Hyperparameters**\n",
    "KNN mainly involves two hyperparameters:\n",
    "-   K value : how many neighbors to participate in the KNN algorithm. k should be tuned based on the validation error.\n",
    "-   distance function : in our case, we choose the Minkowski distance because it allows us to work in a N-D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a0d0e",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f322f",
   "metadata": {},
   "source": [
    "Decision tree is a tree based algorithm used to solve regression and classification problems. An inverted tree is framed which is branched off from a homogeneous probability distributed root node, to highly heterogeneous leaf nodes, for deriving the output.\n",
    "\n",
    "**Algorithm to select conditions**\n",
    "\n",
    "For (classification and regression trees), *Gini index* or *Entropy index* can be used as classification metric. This lets us calculate how well the datapoints are mixed together.\n",
    "\n",
    "**Advantages**\n",
    "-   No preprocessing needed on data.\n",
    "-   No assumptions on distribution of data.\n",
    "-   Handles colinearity efficiently.\n",
    "-   Decision trees can provide understandable explanation over the prediction.\n",
    "\n",
    "**Disadvantages**\n",
    "-   Chances for overfitting the model if we keep on building the tree to achieve high purity. decision tree pruning can be used to solve this issue.\n",
    "-   Prone to outliers.\n",
    "-   Tree may grow to be very complex while training complicated datasets.\n",
    "-   Looses valuable information while handling continuous variables.\n",
    "\n",
    "**Hyperparameters**\n",
    "Decision tree includes many hyperparameters and I will list a few among them.\n",
    "\n",
    "-   **criterion** : which cost function for selecting the next tree node. Mostly used ones are gini/entropy.\n",
    "-   **max depth :** it is the maximum allowed depth of the decision tree.\n",
    "-   **minimum samples split :** It is the minimum nodes required to split an internal node.\n",
    "-   **minimum samples leaf :** minimum samples that are required to be at the leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba11dd",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brief theory recap here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36354b",
   "metadata": {},
   "source": [
    "## Comparison Between Models and Final Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0898ce4",
   "metadata": {},
   "source": [
    "In this paragraph, some considerations about performances and effectiveness are reported with the aim of undestranting the best working conditions for each model.\n",
    "\n",
    "Logistic regression has a convex loss function, so it won't hang in a local minima, whereas for example neaural network may. One important thing to consider is that logistic regression outperforms neural network when training data is less and features are large, since neural networks need large training data. Of course there is a strike also for neural networks since they can support non-linear solutions where for example logistic regression can not. \n",
    "Talking about time consumption, KNN is comparatively slower than other competitors like logistic regression and decision trees, but it supports non-linear solutions too. One major downgrade is that, KNN can only output the labels. Lukily, KNN requires less data to achieve a sufficient accuracy respect to neural networks, but it needs lot of hyperparameter tuning compared to KNN.\n",
    "Finally, let's spend some workd about decision trees. In general, they handle colinearity better, but can not derive the significance of features, hence they are better for a categorical evaluation. Respect to KNN, decision tree supports automatic feature interaction, and it is faster due to KNNâ€™s expensive real time execution. Decision trees perform better when there is a large set of categorical values in the training data. In comparison to neural networks, decision trees are better suited when the scenario demands an explanation over the decision, but when there is sufficient training data, neural networks outperfomr drastically decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d84b4",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d1308",
   "metadata": {},
   "source": [
    "- [Comparative Study on Classic Machine learning Algorithms](https://towardsdatascience.com/comparative-study-on-classic-machine-learning-algorithms-24f9ff6ab222)\n",
    "- [Daily and Sports Activities Dataset](https://archive-beta.ics.uci.edu/ml/datasets/daily+and+sports+activities)\n",
    "- [Scikit Learn Python Library](https://scikit-learn.org/stable/)\n",
    "- [Project GitHub Repository](https://github.com/d-galli/SportActivitiesClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cabdb84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
